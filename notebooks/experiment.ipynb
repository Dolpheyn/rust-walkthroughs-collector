{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in ./.conda/lib/python3.11/site-packages (1.35.9)\n",
      "Requirement already satisfied: langchain in ./.conda/lib/python3.11/site-packages (0.2.6)\n",
      "Requirement already satisfied: faiss-cpu in ./.conda/lib/python3.11/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: langchain-community in ./.conda/lib/python3.11/site-packages (0.2.6)\n",
      "Requirement already satisfied: ollama in ./.conda/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: chromadb in ./.conda/lib/python3.11/site-packages (0.5.3)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.conda/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.conda/lib/python3.11/site-packages (from openai) (2.8.0)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.conda/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in ./.conda/lib/python3.11/site-packages (from langchain) (0.2.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.conda/lib/python3.11/site-packages (from langchain) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.18.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.conda/lib/python3.11/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.conda/lib/python3.11/site-packages (from chromadb) (3.10.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: pyproject_hooks in ./.conda/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in ./.conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in ./.conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in ./.conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in ./.conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in ./.conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in ./.conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (2.2.0)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.31.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: coloredlogs in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.25.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.46b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.5.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.conda/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.conda/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.conda/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in ./.conda/lib/python3.11/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.conda/lib/python3.11/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.conda/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./.conda/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv openai langchain faiss-cpu langchain-community ollama chromadb tqdm\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%reload_ext dotenv\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ: print('`OPENAI_API_KEY` environment variable is missing.')\n",
    "if 'OPENAI_API_BASE_URL' not in os.environ: print('`OPENAI_API_BASE_URL` environment variable is missing.')\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_BASE_URL = os.environ.get(\"OPENAI_API_BASE_URL\")\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "def get_client(base_url=OPENAI_API_BASE_URL, api_key=OPENAI_API_KEY):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=api_key,\n",
    "        api_version=\"2023-03-15-preview\",\n",
    "        base_url=base_url\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./output/contents/https-rust-code-maven-com-multi-crate-project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob('./output/contents/*')\n",
    "print(f\"{len(files)} files\")\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "scraped_files = []\n",
    "for filepath in files:\n",
    "    article = json.loads(open(filepath, 'r').read())\n",
    "    filename = filepath.split('/')[-1]\n",
    "    scraped_files.append((filename, article))\n",
    "\n",
    "len(scraped_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Rust Multi-crate project in a monorepo', 'link': 'https://rust.code-maven.com/multi-crate-project', 'content': \"Rust Multi-crate project in a monorepo\\nAs your project growth at one point your might feel that splitting the code into multiple crates could be a good idea. It might make the\\ncode cleaner and more reusable across projects. You might even decide to publish some of these crates separately.\\nThere are at least two ways to manage this. One is to put each crate in its own repository. It has the advantage of making them totally\\nindependent, but it will create some extra overhead.\\nThe alternative is to create workspaces. Though the Rust documentation does not\\nmention this, this is similar to the idea of using monorepo.\\nLet's see how we go about doing this.\\nThe Cargo.toml file looks like this:\\nWe edit the src/main.rs file to contain:\\nAt this point we can cargo run or cargo test`. There is nothing special about it.\\nThis is how the directory tree looks like:\\nCrate the folder crates in the root of the project.\\nThis command created the crates/first folder and also changed the Cargo.toml in the root of the project adding the workspace entry:\\nIn the crates/first folder there is a new Cargo.toml file with the following default content:\\nThere is also the crates/first/src/lib.rs file that I changed to the following:\\nThis was generated by the cargo new command, but I changed the test a little.\\nThe directory structure looks like this:\\nWhile being in the root of the project:\\ncargo test will run the tests of the main crate only.\\ncargo test --workspace first will run the tests of the first crate.\\ncargo test --all will run the tests of all the crates.\\nHowever, if one of the tests fails then this command will not run any more tests. It stops after the first failure.\\nwe can change that by running\\ncargo test --all --no-fail-fast.\\nYou can experiment with this by changing the assert_eq!(1, 1) to assert_eq!(1, 2) to make the test fail.\\nTyping that command every time is a bit boring. Luckily cargo can have a configuration file to set aliases\\nthat come with the repository:\\nCreate the .cargo folder and in there create the file .cargo/config.toml with the following content:\\nThis will let you type in cargo t.\\nWhile probably most of the internal crates will be libraries occasionally there might be an executable so I wanted to see how\\nthat would work:\\nThe directory tree now looks like this:\\nThis command updated the main Cargo.toml file again, adding another entry to the list of workspaces:\\nIt created a standard crates/second/Crate.toml file and a standard crates/second/src/main.rs file\\nthat I changed a bit:\\nThe alias we created earlier works, so we can run all the tests of all the crates using:\\ncargo run will run the executable of the main crate.\\ncargo run --package second will run the executable of the crate called second.\\nYou can create an alias for that if that makes life easier.\\nAll the creates use the same target folder in the root of the project.\\nWe can change the main Cargo.toml file to include all the crates in the crates folder.\\nWe can also exclude some.\\nAccording to the documentation some things\\ncan be configured globally in the main Cargo.toml file, but for example I had to add the Clippy lint\\nconfiguration to each one of the Cargo.toml files separately.\\nsource\\nGabor Szabo, the author of the Rust Maven web site maintains several Open source projects in Rust\\nand while he still feels he has tons of new things to learn about Rust he already offers training courses in Rust\\nand still teaches Python, Perl, git, GitHub, GitLab, CI, and testing.\\nGet extra content and notifications in the Rust Maven newsletter!. source\"}\n"
     ]
    }
   ],
   "source": [
    "print(scraped_files[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "CLEANED_DIR = './output/contents'\n",
    "def try_get_cleaned_content(fname):\n",
    "    fpath = os.path.join(CLEANED_DIR, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        return None\n",
    "    return json.loads(open(fpath, 'r').read())\n",
    "\n",
    "EMBEDDINGS_DIR = \"./output/embeddings\"\n",
    "def store_embeddings(fname, embeddings):\n",
    "    fpath = os.path.join(EMBEDDINGS_DIR, fname)\n",
    "    import json\n",
    "    open(fpath, 'w').write(json.dumps(embeddings))\n",
    "\n",
    "def get_embedding(text, client, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "import ollama\n",
    "def get_embedding_ollama(text, model=\"mxbai-embed-large\"):\n",
    "    return ollama.embeddings(\n",
    "        model=model,\n",
    "        prompt=text,\n",
    "    )\n",
    "\n",
    "def split_text_into_chunks(text):\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator='\\n',\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "import chromadb\n",
    "def get_chromadb_client():\n",
    "    return chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:06<01:01,  1.49it/s]Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      " 21%|██        | 21/100 [00:16<01:27,  1.10s/it]Created a chunk of size 1130, which is longer than the specified 1000\n",
      "Created a chunk of size 1419, which is longer than the specified 1000\n",
      "Created a chunk of size 2735, which is longer than the specified 1000\n",
      " 27%|██▋       | 27/100 [00:25<01:38,  1.35s/it]Created a chunk of size 1359, which is longer than the specified 1000\n",
      "Created a chunk of size 1097, which is longer than the specified 1000\n",
      "Created a chunk of size 1089, which is longer than the specified 1000\n",
      "Created a chunk of size 1122, which is longer than the specified 1000\n",
      " 32%|███▏      | 32/100 [00:33<01:31,  1.35s/it]Created a chunk of size 1030, which is longer than the specified 1000\n",
      " 44%|████▍     | 44/100 [00:53<01:07,  1.21s/it]Created a chunk of size 1459, which is longer than the specified 1000\n",
      "Created a chunk of size 1088, which is longer than the specified 1000\n",
      " 51%|█████     | 51/100 [01:01<00:46,  1.06it/s]Created a chunk of size 1055, which is longer than the specified 1000\n",
      " 64%|██████▍   | 64/100 [01:22<01:03,  1.76s/it]Created a chunk of size 1221, which is longer than the specified 1000\n",
      " 75%|███████▌  | 75/100 [01:28<00:12,  2.04it/s]Created a chunk of size 1429, which is longer than the specified 1000\n",
      "Created a chunk of size 1373, which is longer than the specified 1000\n",
      "Created a chunk of size 1429, which is longer than the specified 1000\n",
      "Created a chunk of size 1373, which is longer than the specified 1000\n",
      " 78%|███████▊  | 78/100 [01:45<01:13,  3.34s/it]Created a chunk of size 1070, which is longer than the specified 1000\n",
      " 95%|█████████▌| 95/100 [02:00<00:03,  1.43it/s]Created a chunk of size 1200, which is longer than the specified 1000\n",
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "chromadb_client = get_chromadb_client()\n",
    "chromadb_collection = None\n",
    "collection_name = 'rust-walkthrough-articles'\n",
    "try:\n",
    "    chromadb_collection = chromadb_client.create_collection(collection_name)\n",
    "except:\n",
    "    chromadb_collection = chromadb_client.get_collection(collection_name)\n",
    "\n",
    "# embeddings - generate and store\n",
    "for (fname, article) in tqdm(scraped_files[:100]):\n",
    "    maybe_cleaned_article = try_get_cleaned_content(fname)\n",
    "    if maybe_cleaned_article is None:\n",
    "        continue\n",
    "    article = maybe_cleaned_article\n",
    "    text_chunks = split_text_into_chunks(article['content'])\n",
    "    chunked_embeddings = [(fname, i, get_embedding_ollama(chunk)['embedding']) for (i, chunk) in enumerate(text_chunks)]\n",
    "    chromadb_collection.add(\n",
    "        ids=[f\"{fname}-{i}\" for (fname, i, _) in chunked_embeddings],\n",
    "        embeddings=[embedding for (_, _, embedding) in chunked_embeddings],\n",
    "        documents = text_chunks,\n",
    "        metadatas=[{'title': article['title'], 'link': article['link']} for _ in text_chunks]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https-manuel-bernhardt-io-posts-2024-01-26-rust-fearless-concurrency-cats-raspberry-pi-2',\n",
       "  {'link': 'https://manuel.bernhardt.io/posts/2024-01-26-rust-fearless-concurrency-cats-raspberry-pi/',\n",
       "   'title': 'Fearless concurrency with Rust, cats, and a few Raspberry PIs'},\n",
       "  'Let’s go!\\nAny distributed system needs the following building blocks in order to operate:\\ndiscovery: a way to discover other member nodes\\ntransport: a way to communicate (the pipes)\\nprotocol: a common language to understand each other\\nconsensus: a way to agree on the state of things\\nAt first, this definition may look somewhat elaborate, so let’s take some time and talk about these different aspects to demystify them.\\nIn order to talk to other nodes on the network, we need to know about them. There can be various degrees of discovery in a distributed system, ranging from a simple, hard-coded list of IP addresses on each node to full-fledged group membership (the foundation of clustered applications). If you want to learn more about this, you can have a look at this Pink Floyd themed talk on group membership.'),\n",
       " ('https-manuel-bernhardt-io-posts-2024-01-26-rust-fearless-concurrency-cats-raspberry-pi-1',\n",
       "  {'link': 'https://manuel.bernhardt.io/posts/2024-01-26-rust-fearless-concurrency-cats-raspberry-pi/',\n",
       "   'title': 'Fearless concurrency with Rust, cats, and a few Raspberry PIs'},\n",
       "  'In the previous article, we looked at building a blinking Raspberry PI with Rust to help cat owners think about cleaning the cat litter box (and avoid death stares from their feline companions). In this article, we’ll take things a step further and expand the Reminder PIs to a network, allowing to spread them all over the house so that forgetting about this daily cat owner duty will become an almost impossible thing to do. In order to achieve this, we will embrace the principles of Fearless Concurrency made possible by Rust’s memory management paradigm. More specifically, we will:\\nhave a look at the design of the networked system\\nexplore the message passing paradigm between threads with channels\\nlook at shared ownership between threads for graceful shutdown\\nreflect on the design decisions, the tradeoffs of the approach and the meaning of life\\nLet’s go!\\nAny distributed system needs the following building blocks in order to operate:\\ndiscovery: a way to discover other member nodes'),\n",
       " ('https-manuel-bernhardt-io-posts-2024-01-26-rust-fearless-concurrency-cats-raspberry-pi-0',\n",
       "  {'link': 'https://manuel.bernhardt.io/posts/2024-01-26-rust-fearless-concurrency-cats-raspberry-pi/',\n",
       "   'title': 'Fearless concurrency with Rust, cats, and a few Raspberry PIs'},\n",
       "  'Fearless concurrency with Rust, cats, and a few Raspberry PIs - Manuel Bernhardt\\nArticles\\nTalks\\nTestimonials\\nPhotography\\nAbout\\nArticles\\nTalks\\nTestimonials\\nPhotography\\nAbout\\nDesigning a simple clustered application\\nDiscovery\\nTransport\\nProtocol\\nConsensus\\nPutting it all together\\nPassing messages between threads with channels\\nChannels can have multiple senders\\nUsing shared ownership for graceful shutdown\\nConcurrency primitives for multithreaded access\\nA few thoughts\\nDiscovery\\nTransport\\nProtocol\\nConsensus\\nPutting it all together\\nChannels can have multiple senders\\nConcurrency primitives for multithreaded access\\nSomehow I’m never satisfied when a program only runs on one computer. The reassuring feeling of connectedness, the thrill of discovering who else is there on the network, the fear of network instability and the insanity of trying to establish a coherent view of the world on multiple machines are both a source of joy and despair that keep me hooked. But I digress…'),\n",
       " ('https-manuel-bernhardt-io-posts-2024-01-26-rust-fearless-concurrency-cats-raspberry-pi-6',\n",
       "  {'link': 'https://manuel.bernhardt.io/posts/2024-01-26-rust-fearless-concurrency-cats-raspberry-pi/',\n",
       "   'title': 'Fearless concurrency with Rust, cats, and a few Raspberry PIs'},\n",
       "  'After failure detection, consensus is one of my favorite subject matters in the distributed systems field. It certainly helps with the slow and steady decline into insanity.\\nThat being said, in this application, the state is ideally simple. We only need to agree on a timestamp. Time only moves forward (it increases monotonically). Well, except in that one corner in my basement where it moves backwards, and it’s generally not a good idea to stay in there for too long because you may then be crossing your future self on the stairs back up later on, which is awkward. But for our application, we can safely assume that any new version of the state will be the right one. Problem solved!\\nIn summary, the networked version of the Reminder application will have the following modules, each spawning a separate thread:\\nAs you can see, some of the modules need to communicate with one another. Let’s see how we can achieve this with one of the concurrency primitives Rust offers: channels.'),\n",
       " ('https-manuel-bernhardt-io-posts-2024-01-26-rust-fearless-concurrency-cats-raspberry-pi-5',\n",
       "  {'link': 'https://manuel.bernhardt.io/posts/2024-01-26-rust-fearless-concurrency-cats-raspberry-pi/',\n",
       "   'title': 'Fearless concurrency with Rust, cats, and a few Raspberry PIs'},\n",
       "  'For the nodes to talk to one another, we need to define a common language, that is to say:\\na set of messages that make up the language\\na technical representation that can cross network boundaries\\nIn this application, there’s really only two messages that we need: one in order to ask for the current version of the state (the timestamp) and another one to tell others that the state has changed. Using the serde crate, the messages look like this:\\nFurther, we’ll use the bincode crate as encoder / decoder implementation.\\nAfter failure detection, consensus is one of my favorite subject matters in the distributed systems field. It certainly helps with the slow and steady decline into insanity.'),\n",
       " ('https-manuel-bernhardt-io-posts-2024-01-26-rust-fearless-concurrency-cats-raspberry-pi-4',\n",
       "  {'link': 'https://manuel.bernhardt.io/posts/2024-01-26-rust-fearless-concurrency-cats-raspberry-pi/',\n",
       "   'title': 'Fearless concurrency with Rust, cats, and a few Raspberry PIs'},\n",
       "  'For the nodes to exchange data we need a transport medium - the basic plumbing underpinning any kind of networked application. Since we’re fearless (and quite frankly our system isn’t a very high stake system) we’ll use the UDP protocol. Each node will act both as server (to receive and respond to requests) and client (to request the latest state when starting up and to inform other nodes that the state has changed locally).\\nAt this point, we could roll our own socket initialization and listening loops. Or we could use the message-io crate which provides a simple abstraction that allows to focus on messages and sockets.\\nFor the nodes to talk to one another, we need to define a common language, that is to say:\\na set of messages that make up the language\\na technical representation that can cross network boundaries'),\n",
       " ('https-infinyon-com-blog-2024-02-fluvio-deep-causality-rs-52',\n",
       "  {'link': 'https://infinyon.com/blog/2024/02/fluvio-deep-causality-rs/',\n",
       "   'title': 'Real-time Streaming Analytics with Fluvio, DeepCausality, and Rust'},\n",
       "  'One particularity you may encounter in real-time systems is the prevalence of the microservice pattern. While the project’s code examples all show client-side processing, you could equally put a causal model or any other form of processing into a microservice. At least, that is a common conception unless you already have a full Spark cluster deployment. The Fluvio project already supports smart modules that allow you to perform common stream processing tasks such as collecting, transforming, deduplicating, or dispatching messages within the Fluvio cluster. However, the second generation of stateful services takes that concept one step further and allows for composable event-driven data pipelines using web assembly modules. Access to stateful services is currently in private preview, and I had chance to test it out. The technology is outstanding, and with those web assembly modules, you can replace a bunch of microservices. While the stateful service may take some time to mature, I am confident it will be on a similar quality level to the existing system. I recommend Fluvio as a message bus to cut the cloud bill and see how the stateful service evolves to see if your requirements can be met by the new paradigm. Like causal models, it won’t work for everything. Still, when it works, you will discover something truly intriguing that gives you capabilities previously thought unattainable. And you get it at an absurdly low operational cost.'),\n",
       " ('https-manuel-bernhardt-io-posts-2024-01-26-rust-fearless-concurrency-cats-raspberry-pi-13',\n",
       "  {'link': 'https://manuel.bernhardt.io/posts/2024-01-26-rust-fearless-concurrency-cats-raspberry-pi/',\n",
       "   'title': 'Fearless concurrency with Rust, cats, and a few Raspberry PIs'},\n",
       "  'I had a first go at building a clustered version of this project a year ago and back then I opted for using an actor library (Rust has many of those). However, I abandoned this approach, partly because I ran out of time and partly because the actor libraries I tried did not play well together with async methods. This time around I opted for using the primitives built into the language, and it turns out that this made the development of this project quite easy (less than one day of work in total). I think that especially for systems with a small amount of threads / modules, channels are an excellent solution. With a large amount of concurrently running modules that need to communicate with one another, another paradigm would probably be required because passing the channels around could become cumbersome, and having multiple receivers for the same message could also be necessary, which is something that channels don’t support.'),\n",
       " ('https-www-fluvio-io-blog-2021-05-java-client-2',\n",
       "  {'link': 'https://www.fluvio.io/blog/2021/05/java-client/',\n",
       "   'title': 'Embedding Rust code in Java Jar for distribution'},\n",
       "  'Declarative Management: a technology pattern where users declare intent and the system provisions resources as they become available.Reconciliation: the cluster continuously checks system components and brings them to a stable state.Replication: all data streams can have multiple copies to reduce the possibility of data loss during outages.\\nOne-Click Deploy: create a cluster locally fluvio cluster start or login to cloud fluvio cloud login with one simple command.Simple CLI: provision streams, apply stateful computations, produce, consume and more.Native APIs: support for Rust, Node.js, Python, and Java with other languages coming soon.\\nTerms of Use\\n      |\\n      Security\\n      |\\n      Privacy Policy'),\n",
       " ('https-infinyon-com-blog-2024-02-fluvio-deep-causality-rs-19',\n",
       "  {'link': 'https://infinyon.com/blog/2024/02/fluvio-deep-causality-rs/',\n",
       "   'title': 'Real-time Streaming Analytics with Fluvio, DeepCausality, and Rust'},\n",
       "  'With the project structure out of the way, let’s look at the architecture next.\\nThe architecture follows the gateway pattern, meaning applications do not connect to the database directly. Instead, each application creates a QD Client that connects to the QD Gateway. The gateway handles essential tasks such as login/logout of clients. Likewise, the mapping from symbols to unique IDs happens via the Symbol Master Database (SYMDB) service. An application connects via the SYMDB client to the symbol service, resolves symbols it wants to stream data, and then connects to the QD gateway to request data streaming for the resolved symbols.\\nThe communication between the QD client and gateway follows a simple protocol.\\nThe client sends a login message with its client ID to the gateway.\\nA client error message gets returned if the client is already logged in.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = get_embedding_ollama('distributed systems')['embedding']\n",
    "res = chromadb_collection.query(query)\n",
    "[(id, meta, doc) for (id, meta, doc) in zip(res['ids'][0], res['metadatas'][0], res['documents'][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = get_client().chat.completions.create(\n",
    "#  model=\"gpt-35-turbo\", # replace this value with the deployment name you chose when you deployed the associated model.\n",
    "#  messages = [{\n",
    "#      \"role\": \"system\",\n",
    "#      \"content\": \"\"\n",
    "#  },\n",
    "#  {\n",
    "#      \"role\": \"user\",\n",
    "#      \"content\": \"\",\n",
    "#  }],\n",
    "#  temperature=2,\n",
    "#  top_p=0.95,\n",
    "#  frequency_penalty=0,\n",
    "#  presence_penalty=0,\n",
    "#  stop=None)\n",
    "#\n",
    "#print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
