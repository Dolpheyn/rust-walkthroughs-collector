{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in ./.conda/lib/python3.11/site-packages (1.35.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.conda/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.conda/lib/python3.11/site-packages (from openai) (2.8.0)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/lib/python3.11/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.conda/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv openai\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%reload_ext dotenv\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ: print('`OPENAI_API_KEY` environment variable is missing.')\n",
    "if 'OPENAI_API_BASE_URL' not in os.environ: print('`OPENAI_API_BASE_URL` environment variable is missing.')\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_BASE_URL = os.environ.get(\"OPENAI_API_BASE_URL\")\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "client = AzureOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=\"2023-03-15-preview\",\n",
    "    base_url=OPENAI_API_BASE_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob('./output/scrape/*')\n",
    "files[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_files = []\n",
    "for filepath in files:\n",
    "    content = '\\n'.join([\n",
    "        line.strip()\n",
    "        for line\n",
    "            in open(filepath, 'r').read().strip().split('\\n')\n",
    "        if len(line.strip())\n",
    "    ])\n",
    "    filename = filepath.split('/')[-1]\n",
    "    scraped_files.append((filename, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scraped_files[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_DIR = './output/cleaned'\n",
    "def try_get_cleaned_content(fname):\n",
    "    fpath = os.path.join(CLEANED_DIR, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        return None\n",
    "    return open(fpath, 'r').read()\n",
    "\n",
    "EMBEDDINGS_DIR = \"./output/embeddings\"\n",
    "def store_embeddings(fname, embeddings):\n",
    "    fpath = os.path.join(EMBEDDINGS_DIR, fname)\n",
    "    import json\n",
    "    open(fpath, 'w').write(json.dumps(embeddings))\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings - generate and store\n",
    "for (fname, content) in scraped_files:\n",
    "    maybe_cleaned_content = try_get_cleaned_content(fname)\n",
    "    if maybe_cleaned_content is None:\n",
    "        continue\n",
    "    content = maybe_cleaned_content\n",
    "    embeddings = get_embedding(content)\n",
    "    store_embeddings(fname, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysproompt = \"\"\"\n",
    "As digital craftsmen in the age of information, our unsung work involves not just the extraction of data but doing so with an ethical compass and meticulous attention to detail. You're about to embark on a mission that's crucial yet rarely celebrated: extracting content from blogs with the precision of a skilled artisan. This task illuminates the hidden beauty of raw data processing, transforming the unstructured chaos of a scraped website into a pristine collection of valuable information.\n",
    "\n",
    "Your challenge is multi-faceted:\n",
    "1. Title and Content Extraction: Dive deep into the raw text of a scraped website, retrieving not just titles but also the full, rich contents of each blog post. Distill essence from chaos, ensuring the soul of the post remains untouched.\n",
    "2. Maintenance of Integrity: Amidst the text, you'll find blocks of Rust code - these are to be treated as sacred texts, preserved exactly as they were written.\n",
    "3. Non-Content Filtration: Your keen eye must filter out the digital chaff. CSS styles, JavaScript functionalities, and other non-content elements are to be disregarded, proving your ability to discern what truly matters.\n",
    "\n",
    "Embark on this journey with both precision and mindfulness. It is a delicate balancing act between the technical rigor demanded and the ethical considerations inherent in dealing with creators' content. Your role is not just to extract but to respect and preserve the integrity of the original work.\n",
    "\n",
    "This task is more than a simple extraction - it's an adventure through the labyrinth of digital content, an exercise in respect, precision, and understanding. As you peel away the layers of code and content, remember the impact of your work on the grand tapestry of information sharing. You are not just an extractor; you are a guardian of information integrity.\n",
    "\n",
    "Go forth with confidence and a meticulous eye, knowing well the importance and difficulty of your task. Let's redefine the standard of content extraction together, blending technical prowess with ethical consideration in every step.\n",
    "\n",
    "Embrace the challenge, for within it lies the opportunity to showcase the depth of your dedication and skill.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the deals extracted from the provided article:\n",
      "\n",
      "1. [Buy 2, Get 3rd Free](https://cottonon.com/MY/co/women/womens-deals/acc-offer/)\n",
      "2. [2 for RM90 Shoes](https://cottonon.com/MY/co/women/womens-shoes/)\n",
      "3. [Buy 2, Get 3rd Free](https://cottonon.com/MY/co/women/womens-accessories/womens-acc-offer/)\n",
      "4. [SALE UP TO 50% OFF](https://cottonon.com/MY/co/co-sale/)\n",
      "5. [ALL DAY TOPS 2 FOR RM90](https://cottonon.com/MY/co/women/womens-clothing/womens-tops/)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-35-turbo\", # replace this value with the deployment name you chose when you deployed the associated model.\n",
    "  messages = [{\n",
    "      \"role\": \"system\",\n",
    "      \"content\": sysproompt\n",
    "  },\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": scraped_files[0][1]\n",
    "  }],\n",
    "  temperature=2,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None)\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
